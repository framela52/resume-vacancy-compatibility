"""
Извлечение именованных сущностей (NER) из текста резюме с использованием SpaCy.

Этот модуль предоставляет функции для извлечения именованных сущностей, таких как организации,
даты, личности, местоположения и пользовательские навыки из текста резюме с использованием
предварительно обученных моделей SpaCy.
"""
import logging
import re
from typing import Dict, List, Optional, Set, Tuple, Union

logger = logging.getLogger(__name__)

# Глобальные экземпляры моделей для избежания повторной загрузки при каждом вызове
_nlp_models: Dict[str, Optional["spacy.language.Language"]] = {
    "en": None,
    "ru": None,
}


def _get_model(language: str = "en") -> "spacy.language.Language":
    """
    Получить или инициализировать модель SpaCy для указанного языка.

    Args:
        language: Код языка ('en' для английского, 'ru' для русского)

    Returns:
        Инициализированный экземпляр модели SpaCy

    Raises:
        ImportError: Если spaCy не установлен
        RuntimeError: Если не удалось загрузить модель или она не загружена
    """
    global _nlp_models

    # Нормализовать код языка
    lang_map = {
        "english": "en",
        "en": "en",
        "russian": "ru",
        "ru": "ru",
    }
    lang = lang_map.get(language.lower(), "en")

    if _nlp_models.get(lang) is None:
        try:
            import spacy

            # Сопоставление названий моделей
            model_names = {
                "en": "en_core_web_sm",
                "ru": "ru_core_news_sm",
            }

            model_name = model_names.get(lang, "en_core_web_sm")

            logger.info(f"Загрузка модели SpaCy: {model_name} для языка: {lang}")

            try:
                _nlp_models[lang] = spacy.load(model_name)
            except OSError:
                raise RuntimeError(
                    f"Модель SpaCy '{model_name}' не найдена. "
                    f"Загрузите её с помощью: python -m spacy download {model_name}"
                )

            logger.info(f"Модель SpaCy {model_name} успешно загружена")

        except ImportError as e:
            raise ImportError(
                "SpaCy не установлен. Установите его с помощью: pip install spacy"
            ) from e
        except Exception as e:
            raise RuntimeError(f"Не удалось загрузить модель SpaCy: {e}") from e

    return _nlp_models[lang]


def extract_entities(
    text: str,
    *,
    language: str = "en",
    entity_types: Optional[Union[List[str], Set[str]]] = None,
    include_custom_skills: bool = True,
) -> Dict[str, Optional[Union[Dict[str, List[Dict[str, Union[str, int, Tuple[int, int]]]]], str]]]:
    """
    Извлечь именованные сущности из текста резюме с использованием SpaCy.

    Эта функция использует предварительно обученные NER-модели SpaCy для извлечения сущностей, таких как:
    - ORG: Организации, компании, учреждения
    - DATE: Даты, периоды, временные выражения
    - PERSON: Имена людей
    - GPE: Географические и политические сущности (страны, города)
    - PRODUCT: Продукты, приложения, программное обеспечение
    - Пользовательские навыки из технических ключевых слов

    Args:
        text: Входной текст для извлечения сущностей
        language: Язык документа ('en', 'english', 'ru', 'russian')
        entity_types: Список типов извлекаемых сущностей (по умолчанию: все стандартные типы)
            - None или пустой список извлекает все стандартные типы
            - Примеры: ['ORG', 'DATE', 'PERSON'], {'ORG', 'DATE'}
        include_custom_skills: Извлекать ли технические навыки с помощью сопоставления по шаблону

    Returns:
        Словарь, содержащий:
            - entities: Словарь, сопоставляющий типы сущностей со списками сущностей:
                - text: Текст сущности
                - label: Метка типа сущности
                - start: Начальная позиция символа
                - end: Конечная позиция символа
                - count: Количество появлений сущности
            - skills: Список обнаруженных технических навыков (если включено)
            - total_count: Общее количество извлечённых сущностей
            - language: Использованный код языка
            - error: Сообщение об ошибке, если извлечение не удалось

    Raises:
        ValueError: Если текст пустой
        RuntimeError: Если не удалось загрузить модель или выполнить извлечение

    Examples:
        >>> text = "Иван Иванов работал в Google с 2019 по 2022. Навыки: Python, Django."
        >>> result = extract_entities(text)
        >>> print(result["entities"]["PERSON"])
        [{'text': 'Иван Иванов', 'label': 'PERSON', 'start': 0, 'end': 11}]
        >>> print(result["entities"]["ORG"])
        [{'text': 'Google', 'label': 'ORG', 'start': 23, 'end': 29}]
        >>> print(result["entities"]["DATE"])
        [{'text': '2019', 'label': 'DATE'}, {'text': '2022', 'label': 'DATE'}]

        Извлечь только определённые типы сущностей:
        >>> result = extract_entities(text, entity_types=['ORG', 'DATE'])

        Извлечь из русского текста:
        >>> result = extract_entities(russian_text, language='ru')
    """
    # Проверить ввод
    if not text or not isinstance(text, str):
        return {
            "entities": None,
            "skills": None,
            "total_count": 0,
            "language": language,
            "error": "Текст должен быть непустой строкой",
        }

    text = text.strip()
    if len(text) < 5:
        return {
            "entities": None,
            "skills": None,
            "total_count": 0,
            "language": language,
            "error": "Текст слишком короткий для извлечения сущностей (минимум 5 символов)",
        }

    # Типы сущностей по умолчанию для извлечения
    if entity_types is None or len(entity_types) == 0:
        entity_types = {"ORG", "DATE", "PERSON", "GPE", "PRODUCT", "EVENT", "WORK_OF_ART"}
    elif isinstance(entity_types, list):
        entity_types = set(entity_types)

    try:
        # Получить или инициализировать модель
        nlp = _get_model(language)

        # Обработать текст
        logger.info(
            f"Извлечение сущностей из текста (длина={len(text)}, язык={language}, "
            f"entity_types={entity_types})"
        )

        doc = nlp(text)

        # Извлечь сущности по типам
        entities_dict: Dict[str, List[Dict[str, Union[str, int, Tuple[int, int]]]]] = {
            et: [] for et in entity_types
        }

        entity_counter: Dict[str, int] = {}

        for ent in doc.ents:
            if ent.label_ in entity_types:
                entity_data = {
                    "text": ent.text,
                    "label": ent.label_,
                    "start": ent.start_char,
                    "end": ent.end_char,
                }

                entities_dict[ent.label_].append(entity_data)

                # Подсчитать появления сущностей
                entity_key = f"{ent.label_}:{ent.text.lower()}"
                entity_counter[entity_key] = entity_counter.get(entity_key, 0) + 1

        # Добавить количество к каждой сущности
        for etype, entities in entities_dict.items():
            for entity in entities:
                entity_key = f"{etype}:{entity['text'].lower()}"
                entity["count"] = entity_counter.get(entity_key, 1)

        # Удалить пустые типы сущностей
        entities_dict = {
            etype: entities
            for etype, entities in entities_dict.items()
            if entities
        }

        # Извлечь пользовательские навыки, если включено
        skills = None
        if include_custom_skills:
            skills = _extract_technical_skills(text, language)
            if skills:
                entities_dict["SKILL"] = [
                    {
                        "text": skill,
                        "label": "SKILL",
                        "start": -1,  # На основе шаблона, без позиции
                        "end": -1,
                        "count": text.lower().count(skill.lower()),
                    }
                    for skill in skills
                ]

        # Подсчитать общее количество сущностей
        total_count = sum(len(entities) for entities in entities_dict.values())

        logger.info(f"Извлечено {total_count} сущностей из текста")

        return {
            "entities": entities_dict if entities_dict else None,
            "skills": skills,
            "total_count": total_count,
            "language": language,
            "error": None,
        }

    except ImportError as e:
        logger.error(f"Ошибка импорта при извлечении сущностей: {e}")
        return {
            "entities": None,
            "skills": None,
            "total_count": 0,
            "language": language,
            "error": f"Ошибка импорта: {str(e)}",
        }
    except Exception as e:
        logger.error(f"Не удалось извлечь сущности: {e}")
        return {
            "entities": None,
            "skills": None,
            "total_count": 0,
            "language": language,
            "error": f"Извлечение не удалось: {str(e)}",
        }


def _extract_technical_skills(
    text: str,
    language: str = "en"
) -> List[str]:
    """
    Extract technical skills using pattern matching.

    This function uses regex patterns to identify common technical skills,
    programming languages, frameworks, and tools in resume text.

    Args:
        text: Resume text to extract skills from
        language: Document language ('en' or 'ru')

    Returns:
        List of unique technical skills found in text
    """
    # Common technical skill patterns
    skill_patterns = [
        # Programming languages
        r'\b(Python|Java|JavaScript|TypeScript|C\+\+|C#|Go|Rust|PHP|Ruby|Swift|Kotlin|Scala|R|MATLAB)\b',
        # Web frameworks
        r'\b(React|Angular|Vue|Django|Flask|Spring\.js|Express|Node\.js|Next\.js|Nuxt\.js)\b',
        # Databases
        r'\b(PostgreSQL|MySQL|MongoDB|Redis|Elasticsearch|SQLite|Oracle|SQL Server|Cassandra)\b',
        # Cloud platforms
        r'\b(AWS|Azure|GCP|Google Cloud|Heroku|DigitalOcean|Vercel|Netlify)\b',
        # DevOps tools
        r'\b(Docker|Kubernetes|Jenkins|GitLab CI|GitHub Actions|CircleCI|Travis CI|Terraform|Ansible)\b',
        # Tools and libraries
        r'\b(Git|GitHub|GitLab|Bitbucket|Jira|Confluence|Slack|VS Code|IntelliJ|PyCharm)\b',
        # Data science/ML
        r'\b(TensorFlow|PyTorch|Keras|Scikit-learn|Pandas|NumPy|Matplotlib|Jupyter|Tableau)\b',
        # Other common skills
        r'\b(REST API|GraphQL|gRPC|Microservices|CI/CD|TDD|Agile|Scrum|Kanban)\b',
        # Version specific
        r'\b(Java \d+|Python 3\.\d+|Node\.js \d+|React \d+)\b',
    ]

    text_lower = text.lower()
    found_skills: Set[str] = set()

    for pattern in skill_patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            skill = match.group()
            # Add both original and lowercase
            found_skills.add(skill)

    # Additional skill extraction from common sections
    skill_section_patterns = [
        r'(?:Skills|Technical Skills|Technologies|Tech Stack|Stack):?\s*([^\n]+)',
        r'(?:Навыки|Технические навыки|Стек технологий):?\s*([^\n]+)',
    ]

    for pattern in skill_section_patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            skills_text = match.group(1)
            # Split by common separators
            potential_skills = re.split(r'[,;·•\-\n]', skills_text)
            for skill in potential_skills:
                skill = skill.strip()
                if len(skill) > 1 and len(skill) < 50:  # Reasonable skill length
                    found_skills.add(skill)

    return sorted(list(found_skills), key=lambda x: x.lower())


def extract_organizations(
    text: str,
    language: str = "en"
) -> Dict[str, Optional[Union[List[str], str]]]:
    """
    Extract organization names from resume text.

    This is a convenience function that extracts only ORG entities,
    such as company names, institutions, and organizations.

    Args:
        text: Resume text to extract organizations from
        language: Document language ('en' or 'ru')

    Returns:
        Dictionary containing:
            - organizations: List of unique organization names
            - count: Number of organizations found
            - error: Error message if extraction failed

    Examples:
        >>> text = "Worked at Google, Microsoft, and Amazon."
        >>> result = extract_organizations(text)
        >>> print(result["organizations"])
        ['Google', 'Microsoft', 'Amazon']
    """
    result = extract_entities(
        text,
        language=language,
        entity_types=["ORG"],
        include_custom_skills=False
    )

    org_entities = result.get("entities", {}).get("ORG", [])
    organizations = list(set([org["text"] for org in org_entities]))

    return {
        "organizations": organizations if organizations else None,
        "count": len(organizations),
        "error": result.get("error"),
    }


def extract_dates(
    text: str,
    language: str = "en"
) -> Dict[str, Optional[Union[List[str], str]]]:
    """
    Extract date expressions from resume text.

    This is a convenience function that extracts only DATE entities,
    such as years, months, date ranges, and time periods.

    Args:
        text: Resume text to extract dates from
        language: Document language ('en' or 'ru')

    Returns:
        Dictionary containing:
            - dates: List of unique date expressions
            - count: Number of dates found
            - error: Error message if extraction failed

    Examples:
        >>> text = "Worked from January 2019 to December 2022"
        >>> result = extract_dates(text)
        >>> print(result["dates"])
        ['January 2019', 'December 2022']
    """
    result = extract_entities(
        text,
        language=language,
        entity_types=["DATE"],
        include_custom_skills=False
    )

    date_entities = result.get("entities", {}).get("DATE", [])
    dates = list(set([date["text"] for date in date_entities]))

    return {
        "dates": dates if dates else None,
        "count": len(dates),
        "error": result.get("error"),
    }


def extract_resume_entities(
    resume_text: str,
    language: str = "en"
) -> Dict[str, Optional[Union[Dict[str, List[Dict[str, Union[str, int, Tuple[int, int]]]]], List[str], str]]]:
    """
    Extract all relevant entities from resume text.

    This function runs entity extraction optimized for resumes, extracting:
    - Organizations (companies, institutions)
    - Dates (work periods, graduation dates)
    - Persons (references, team members)
    - Locations (work locations)
    - Technical skills

    Args:
        resume_text: Full resume text
        language: Document language ('en' or 'ru')

    Returns:
        Dictionary containing:
            - organizations: List of organization names
            - dates: List of date expressions
            - persons: List of person names
            - locations: List of geographical entities
            - skills: List of technical skills
            - all_entities: Complete entity dict with positions
            - error: Error message if extraction failed

    Examples:
        >>> result = extract_resume_entities(resume_text, language="en")
        >>> print(result["organizations"])
        ['Google', 'Microsoft']
        >>> print(result["skills"])
        ['Python', 'Django', 'PostgreSQL']
    """
    try:
        result = extract_entities(
            resume_text,
            language=language,
            entity_types=["ORG", "DATE", "PERSON", "GPE", "PRODUCT"],
            include_custom_skills=True
        )

        if result.get("error"):
            return {
                "organizations": None,
                "dates": None,
                "persons": None,
                "locations": None,
                "skills": None,
                "all_entities": None,
                "error": result["error"],
            }

        entities_dict = result.get("entities", {})

        # Extract unique entities by type
        org_list = list(set([e["text"] for e in entities_dict.get("ORG", [])]))
        date_list = list(set([e["text"] for e in entities_dict.get("DATE", [])]))
        person_list = list(set([e["text"] for e in entities_dict.get("PERSON", [])]))
        location_list = list(set([e["text"] for e in entities_dict.get("GPE", [])]))
        skill_list = result.get("skills", [])

        return {
            "organizations": org_list if org_list else None,
            "dates": date_list if date_list else None,
            "persons": person_list if person_list else None,
            "locations": location_list if location_list else None,
            "skills": skill_list if skill_list else None,
            "all_entities": entities_dict if entities_dict else None,
            "error": None,
        }

    except Exception as e:
        logger.error(f"Resume entity extraction failed: {e}")
        return {
            "organizations": None,
            "dates": None,
            "persons": None,
            "locations": None,
            "skills": None,
            "all_entities": None,
            "error": f"Extraction failed: {str(e)}",
        }
